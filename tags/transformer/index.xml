<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>transformer - Tag - Sayef&#39;s Tech Blog</title>
        <link>https://sayef.github.io/tags/transformer/</link>
        <description>transformer - Tag - Sayef&#39;s Tech Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>hello@sayef.tech (Sayef)</managingEditor>
            <webMaster>hello@sayef.tech (Sayef)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Wed, 12 Jun 2019 04:15:37 &#43;0800</lastBuildDate><atom:link href="https://sayef.github.io/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Attention Is All You Need: The Transformer</title>
    <link>https://sayef.github.io/post/attention-is-all-you-need/</link>
    <pubDate>Wed, 12 Jun 2019 04:15:37 &#43;0800</pubDate>
    <author>Sayef</author>
    <guid>https://sayef.github.io/post/attention-is-all-you-need/</guid>
    <description><![CDATA[Introduction The advent of deep learning over the past few years has opened a lot of possibilities regarding neural machine translation (NMT). Attention is all you need, also known as Transformer [1], has become the state-of-the-art in NMT, surpassing tradition recurrent neural network (RNN) based encoder-decoder architecture. A bunch of new architectures is now being built based on the transformer. I will discuss how NMT has evolved throughout the last couple of years, from the traditional RNN to the Transformer.]]></description>
</item>
</channel>
</rss>
